# ── Base model ────────────────────────────────────────────────────────
BASE_MODEL_NAME=meta-llama/Llama-3.2-1B-Instruct
HF_TOKEN=hf_your_token_here
HF_CACHE_DIR=~/.cache/huggingface

# ── LoRA ──────────────────────────────────────────────────────────────
LORA_R=16
LORA_ALPHA=32
LORA_DROPOUT=0.05
LORA_TARGET_MODULES=q_proj,v_proj,k_proj,o_proj,gate_proj,up_proj,down_proj
LORA_BIAS=none                     # none | all | lora_only

# ── QLoRA ─────────────────────────────────────────────────────────────
LOAD_IN_4BIT=true
BNB_4BIT_COMPUTE_DTYPE=bfloat16   # float16 | bfloat16
BNB_4BIT_QUANT_TYPE=nf4           # nf4 | fp4
BNB_USE_DOUBLE_QUANT=true

# ── Training ──────────────────────────────────────────────────────────
OUTPUT_DIR=./ft-outputs
EPOCHS=3
BATCH_SIZE=4
GRADIENT_ACCUMULATION_STEPS=4
LEARNING_RATE=2e-4
WEIGHT_DECAY=0.01
WARMUP_RATIO=0.03
MAX_SEQ_LENGTH=2048
MIXED_PRECISION=bf16              # bf16 | fp16 | no
GRADIENT_CLIPPING=1.0
SEED=42
SAVE_STEPS=100
EVAL_STEPS=100
LOGGING_STEPS=10
SAVE_TOTAL_LIMIT=3

# ── Data ──────────────────────────────────────────────────────────────
DATASET_NAME=tatsu-lab/alpaca
DATASET_SPLIT=train
INSTRUCTION_COLUMN=instruction
INPUT_COLUMN=input
OUTPUT_COLUMN=output

# ── DPO ───────────────────────────────────────────────────────────────
DPO_BETA=0.1

# ── Tracking ─────────────────────────────────────────────────────────
WANDB_PROJECT=finetuning
WANDB_API_KEY=
MLFLOW_TRACKING_URI=http://localhost:5000
