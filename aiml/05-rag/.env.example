# ── Embeddings ────────────────────────────────────────────────────────
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
EMBEDDING_DEVICE=auto              # auto | cuda | cpu
EMBEDDING_BATCH_SIZE=64

# ── Chunking ──────────────────────────────────────────────────────────
CHUNK_SIZE=512
CHUNK_OVERLAP=64
CHUNKING_STRATEGY=recursive        # recursive | sentence | token | fixed

# ── ChromaDB (local) ─────────────────────────────────────────────────
CHROMA_PERSIST_DIR=./chroma_db
CHROMA_COLLECTION_NAME=rag_docs

# ── FAISS (local) ────────────────────────────────────────────────────
FAISS_INDEX_PATH=./faiss_index
FAISS_INDEX_TYPE=Flat              # Flat | IVF | HNSW

# ── Pinecone (online) ─────────────────────────────────────────────────
PINECONE_API_KEY=your_pinecone_api_key
PINECONE_INDEX_NAME=rag-index
PINECONE_CLOUD=aws
PINECONE_REGION=us-east-1
PINECONE_NAMESPACE=default

# ── Retrieval ────────────────────────────────────────────────────────
TOP_K=5
SIMILARITY_THRESHOLD=0.0
USE_RERANKER=false
RERANKER_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2

# ── LLM for generation ───────────────────────────────────────────────
LLM_PROVIDER=groq                  # groq | gemini | ollama | cerebras | openai
LLM_MODEL=llama-3.1-70b-versatile
LLM_TEMPERATURE=0.1
LLM_MAX_TOKENS=2048

# ── LLM Provider Keys ────────────────────────────────────────────────
GROQ_API_KEY=gsk_your_groq_key
GOOGLE_API_KEY=your_gemini_key
OLLAMA_BASE_URL=http://localhost:11434
CEREBRAS_API_KEY=your_cerebras_key
OPENAI_API_KEY=sk_your_openai_key
